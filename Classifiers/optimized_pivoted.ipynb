{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3625ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "Random Forest Best Params: {'classifier__class_weight': None, 'classifier__max_depth': 10, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 100}\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "SVM Best Params: {'classifier__C': 10, 'classifier__gamma': 0.1}\n",
      "=== Random Forest Evaluation ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.80      0.88      1058\n",
      "           1       0.04      0.62      0.08        16\n",
      "\n",
      "    accuracy                           0.79      1074\n",
      "   macro avg       0.52      0.71      0.48      1074\n",
      "weighted avg       0.98      0.79      0.87      1074\n",
      "\n",
      "Confusion Matrix:\n",
      "[[843 215]\n",
      " [  6  10]]\n",
      "\n",
      "\n",
      "=== SVM Evaluation ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.44      0.61      1058\n",
      "           1       0.02      0.94      0.05        16\n",
      "\n",
      "    accuracy                           0.45      1074\n",
      "   macro avg       0.51      0.69      0.33      1074\n",
      "weighted avg       0.98      0.45      0.60      1074\n",
      "\n",
      "Confusion Matrix:\n",
      "[[466 592]\n",
      " [  1  15]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Data Loading & Preprocessing\n",
    "# =============================================================================\n",
    "def load_data(filepath):\n",
    "    \"\"\"Load the cleaned/pivoted data from CSV file.\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    return df\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"Clean the data: handle infinities and missing values.\"\"\"\n",
    "    # Replace infinity with NaN, then fill with median values\n",
    "    df = df.drop(columns = ['Name'])\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Feature Engineering\n",
    "# =============================================================================\n",
    "def compute_altman_zscore(df):\n",
    "    \"\"\"\n",
    "    Compute Altman Z-score from available columns.\n",
    "    \n",
    "    **Note:** The classical Altman Z-Score for manufacturing firms:\n",
    "      Z = 1.2*(WC/TA) + 1.4*(RE/TA) + 3.3*(EBIT/TA) + 0.6*(MVE/TL) + 1.0*(Sales/TA)\n",
    "      \n",
    "    Here we make assumptions for proxies:  \n",
    "      - Working Capital (WC): Use \"Cash balance_11\" as a proxy  \n",
    "      - Total Assets (TA): Sum of \"Net property, plant and equipment_11\" and \"Long term investments_11\"  \n",
    "      - Retained Earnings (RE): Use \"Profit after tax reported by company_11\"  \n",
    "      - EBIT: Use \"PBIT_11\"  \n",
    "      - Market Value of Equity (MVE): Use \"Total income_11\" as a proxy  \n",
    "      - Total Liabilities (TL): Use \"Long term loans & advances_11\"  \n",
    "      - Sales: Use \"Sales_11\"\n",
    "    \n",
    "    Adjust these assumptions as needed!\n",
    "    \"\"\"\n",
    "    # Use the .get method to avoid KeyErrors\n",
    "    WC = df.get(\"Cash balance_11\", pd.Series(0))\n",
    "    TA = df.get(\"Net property, plant and equipment_11\", pd.Series(0)) + df.get(\"Long term investments_11\", pd.Series(0))\n",
    "    RE = df.get(\"Profit after tax reported by company_11\", pd.Series(0))\n",
    "    EBIT = df.get(\"PBIT_11\", pd.Series(0))\n",
    "    MVE = df.get(\"Total income_11\", pd.Series(0))\n",
    "    TL = df.get(\"Long term loans & advances_11\", pd.Series(0))\n",
    "    Sales = df.get(\"Sales_11\", pd.Series(0))\n",
    "    \n",
    "    # Prevent division by zero\n",
    "    TA = TA.replace(0, np.nan)\n",
    "    TL = TL.replace(0, np.nan)\n",
    "    \n",
    "    # Calculate Z-score; fill NaN later with 0\n",
    "    z = 1.2 * (WC / TA) + 1.4 * (RE / TA) + 3.3 * (EBIT / TA) + 0.6 * (MVE / TL) + 1.0 * (Sales / TA)\n",
    "    z.fillna(0, inplace=True)\n",
    "    return z\n",
    "\n",
    "def add_new_features(df):\n",
    "    \"\"\"Add new features such as growth rates and the Altman Z-score.\"\"\"\n",
    "    # Example: Create growth features using the first and last year's values.\n",
    "    # Adjust column names if necessary.\n",
    "    if (\"Total income_1\" in df.columns) and (\"Total income_11\" in df.columns):\n",
    "        df[\"Income_Growth\"] = np.where(df[\"Total income_1\"] == 0, 0, \n",
    "                                       (df[\"Total income_11\"] - df[\"Total income_1\"]) / df[\"Total income_1\"])\n",
    "    if (\"Profit after tax_1\" in df.columns) and (\"Profit after tax_11\" in df.columns):\n",
    "        df[\"Profit_Growth\"] = np.where(df[\"Profit after tax_1\"] == 0, 0, \n",
    "                                       (df[\"Profit after tax_11\"] - df[\"Profit after tax_1\"]) / df[\"Profit after tax_1\"])\n",
    "    \n",
    "    # Add Altman Z-score\n",
    "    df[\"Altman_Z\"] = compute_altman_zscore(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Feature Vector Selection\n",
    "# =============================================================================\n",
    "def select_features(df):\n",
    "    \"\"\"\n",
    "    Select a list of key features for the classification.\n",
    "    Adjust this list based on your exploratory analysis.\n",
    "    \"\"\"\n",
    "    feature_cols = [\n",
    "        \"Total income_11\", \"Sales_11\", \"Net sales_11\", \"Profit after tax_11\",\n",
    "        \"Sales / Net fixed assets_11\", \"Current ratio_11\", \"Quick ratio_11\",\n",
    "        \"Cash to current liabilities (times)_11\", \"Income_Growth\", \"Profit_Growth\",\n",
    "        \"Altman_Z\"\n",
    "    ]\n",
    "    # Ensure that these columns exist in the data\n",
    "    available_features = [col for col in feature_cols if col in df.columns]\n",
    "    return available_features\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Model Building and Hyperparameter Optimization\n",
    "# =============================================================================\n",
    "def build_and_optimize_models(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Build and optimize models (Random Forest and SVM) using grid search.\n",
    "    Returns the best estimators for both models.\n",
    "    \"\"\"\n",
    "    # ---- Random Forest Pipeline ----\n",
    "    pipeline_rf = ImbPipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"classifier\", RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "    ])\n",
    "    \n",
    "    param_grid_rf = {\n",
    "        \"classifier__n_estimators\": [100, 200, 300],\n",
    "        \"classifier__max_depth\": [None, 10, 20],\n",
    "        \"classifier__min_samples_split\": [2, 5, 10],\n",
    "        \"classifier__class_weight\": [None, \"balanced\"]\n",
    "    }\n",
    "    \n",
    "    grid_rf = GridSearchCV(pipeline_rf, param_grid_rf, cv=5, scoring=\"f1\", n_jobs=-1, verbose=1)\n",
    "    grid_rf.fit(X_train, y_train)\n",
    "    print(\"Random Forest Best Params:\", grid_rf.best_params_)\n",
    "    \n",
    "    # ---- SVM Pipeline ----\n",
    "    pipeline_svm = ImbPipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"classifier\", SVC(kernel=\"rbf\", class_weight=\"balanced\", probability=True, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    param_grid_svm = {\n",
    "        \"classifier__C\": [0.1, 1, 10],\n",
    "        \"classifier__gamma\": [0.001, 0.01, 0.1]\n",
    "    }\n",
    "    \n",
    "    grid_svm = GridSearchCV(pipeline_svm, param_grid_svm, cv=5, scoring=\"f1\", n_jobs=-1, verbose=1)\n",
    "    grid_svm.fit(X_train, y_train)\n",
    "    print(\"SVM Best Params:\", grid_svm.best_params_)\n",
    "    \n",
    "    return grid_rf.best_estimator_, grid_svm.best_estimator_\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name=\"Model\"):\n",
    "    \"\"\"Evaluate the model performance on the test set.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"=== {model_name} Evaluation ===\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# 5. Main Execution\n",
    "# =============================================================================\n",
    "def main():\n",
    "    # -- Load and Clean Data --\n",
    "    filepath = \"pivoted_financial_data.csv\"  # Update to your file path if needed\n",
    "    df = load_data(filepath)\n",
    "    df = clean_data(df)\n",
    "    # -- Feature Engineering --\n",
    "    df = add_new_features(df)\n",
    "    features = select_features(df)\n",
    "    \n",
    "    # -- Prepare Feature Matrix and Target Vector --\n",
    "    X = df[features]\n",
    "    y = df[\"Label\"]\n",
    "    \n",
    "    # -- Split Data with Stratification --\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    # -- Build & Optimize Models --\n",
    "    best_rf, best_svm = build_and_optimize_models(X_train, y_train)\n",
    "    \n",
    "    # -- Evaluate the models --\n",
    "    evaluate_model(best_rf, X_test, y_test, model_name=\"Random Forest\")\n",
    "    evaluate_model(best_svm, X_test, y_test, model_name=\"SVM\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
